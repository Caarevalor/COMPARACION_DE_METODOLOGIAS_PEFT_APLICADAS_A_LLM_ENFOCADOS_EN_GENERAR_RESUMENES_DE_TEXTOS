{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "TOKENIZER_NAME = \"openai-community/gpt2\"\n",
    "# TOKENIZER_NAME = \"gpt2-medium\"\n",
    "# TOKENIZER_NAME = \"openai-community/gpt2\"\n",
    "MODEL_NAME = f\"./{TOKENIZER_NAME}-fine-tuned-model\"\n",
    "# MODEL_NAME = \"./checkpoing/checkpoint-300\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Formatting libraries\n",
    "import black\n",
    "import jupyter_black\n",
    "\n",
    "# Load jupyter_black settings\n",
    "jupyter_black.load(\n",
    "    lab=True,\n",
    "    line_length=170,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"daily_cnn\",1.)\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define tokenizer. We will use the tokenizer to count the number of tokens per instance\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, padding_side=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 287113\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 13368\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 11490\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define prompt template\n",
    "prompt_template = \"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "### Conversation:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "### Summary:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# create prompt\n",
    "def create_prompt(data):\n",
    "    dialogue = data[\"article\"]\n",
    "    summary = data[\"highlights\"]\n",
    "    prompt = prompt_template.format(dialogue=dialogue, summary=summary)\n",
    "\n",
    "    n_tokens_output = len(tokenizer.encode(summary, add_special_tokens=False))\n",
    "    n_tokens_input = len(tokenizer.encode(prompt, add_special_tokens=False))\n",
    "\n",
    "    return {\"input\": prompt, \"output\": summary, \"n_tokens_input\": n_tokens_input, \"n_tokens_output\": n_tokens_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'highlights', 'id', 'input', 'output', 'n_tokens_input', 'n_tokens_output'],\n",
       "        num_rows: 287113\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['article', 'highlights', 'id', 'input', 'output', 'n_tokens_input', 'n_tokens_output'],\n",
       "        num_rows: 13368\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'highlights', 'id', 'input', 'output', 'n_tokens_input', 'n_tokens_output'],\n",
       "        num_rows: 11490\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(create_prompt)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "991.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get 0.95 percentile of dialogue length in training set\n",
    "dataset[\"train\"].to_pandas().n_tokens_input.quantile(0.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get 0.95 quantile of n_tokens_summary in train dataset`\n",
    "dataset[\"train\"].to_pandas().n_tokens_output.quantile(0.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens_input</th>\n",
       "      <th>n_tokens_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>287113.000000</td>\n",
       "      <td>287113.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>889.020476</td>\n",
       "      <td>65.692079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>423.328305</td>\n",
       "      <td>27.458405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>576.000000</td>\n",
       "      <td>49.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>814.000000</td>\n",
       "      <td>62.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1119.000000</td>\n",
       "      <td>76.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4696.000000</td>\n",
       "      <td>2426.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       n_tokens_input  n_tokens_output\n",
       "count   287113.000000    287113.000000\n",
       "mean       889.020476        65.692079\n",
       "std        423.328305        27.458405\n",
       "min         34.000000         5.000000\n",
       "25%        576.000000        49.000000\n",
       "50%        814.000000        62.000000\n",
       "75%       1119.000000        76.000000\n",
       "max       4696.000000      2426.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].to_pandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filter very long dialogs and summaries\n",
    "dataset = dataset.filter(lambda x: x[\"n_tokens_input\"] < 942 and x[\"n_tokens_output\"] < 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens_input</th>\n",
       "      <th>n_tokens_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>128837.000000</td>\n",
       "      <td>128837.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>599.438182</td>\n",
       "      <td>50.412894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>193.010698</td>\n",
       "      <td>11.966592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>46.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>451.000000</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>604.000000</td>\n",
       "      <td>51.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>757.000000</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>941.000000</td>\n",
       "      <td>69.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       n_tokens_input  n_tokens_output\n",
       "count   128837.000000    128837.000000\n",
       "mean       599.438182        50.412894\n",
       "std        193.010698        11.966592\n",
       "min         46.000000         6.000000\n",
       "25%        451.000000        43.000000\n",
       "50%        604.000000        51.000000\n",
       "75%        757.000000        60.000000\n",
       "max        941.000000        69.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].to_pandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens_input</th>\n",
       "      <th>n_tokens_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>128837.000000</td>\n",
       "      <td>128837.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>599.438182</td>\n",
       "      <td>50.412894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>193.010698</td>\n",
       "      <td>11.966592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>46.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>451.000000</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>604.000000</td>\n",
       "      <td>51.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>757.000000</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>941.000000</td>\n",
       "      <td>69.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       n_tokens_input  n_tokens_output\n",
       "count   128837.000000    128837.000000\n",
       "mean       599.438182        50.412894\n",
       "std        193.010698        11.966592\n",
       "min         46.000000         6.000000\n",
       "25%        451.000000        43.000000\n",
       "50%        604.000000        51.000000\n",
       "75%        757.000000        60.000000\n",
       "max        941.000000        69.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].to_pandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "### Conversation:\n",
      "\n",
      "LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don't think I'll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he'll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I'll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe's earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say 'kid star goes off the rails,'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter's latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer's \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he's legally an adult: \"I just think I'm going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.\n",
      "\n",
      "### Summary:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][\"input\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\n",
      "Young actor says he has no plans to fritter his cash away .\n",
      "Radcliffe's earnings from first five Potter films have been held in trust fund .\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][\"output\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load fine-tined model and prepare Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from utils import LLMInference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special Tokens: \n",
      "{'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, add_special_tokens=False)\n",
    "\n",
    "# verify the existing special tokens\n",
    "print(f\"Special Tokens: \\n{tokenizer.special_tokens_map}\")\n",
    "\n",
    "# if no padding token set eos_token as padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load model and tokenizer\n",
    "MODEL_NAME = \".\\openai-community\\gpt2-fine-tuned-model\"\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"cuda\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\", device_map=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union\n",
    "from transformers import GenerationConfig\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import os\n",
    "import transformers\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_inference = LLMInference(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "# Cargar la métrica BLEU\n",
    "bleu_metric = load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29431bdb1395470585de05d3bdabb6b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing BLEU scores\n",
      "Computing ROUGE scores\n"
     ]
    }
   ],
   "source": [
    "llm_inference.make_predictions_and_compute_metrics(\n",
    "    dataset=dataset[\"test\"],\n",
    "    batch_size=15,\n",
    "    source_max_len=942,\n",
    "    padding_side=\"left\",\n",
    "    max_new_tokens=70,\n",
    "    do_sample=False,\n",
    "    temperature=0.2,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.0,\n",
    "    # generation_config_kwargs={\"num_beams\": 5},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bleu_score</th>\n",
       "      <th>1-gram precision</th>\n",
       "      <th>2-gram precision</th>\n",
       "      <th>3-gram precision</th>\n",
       "      <th>4-gram precision</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.109</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bleu_score  1-gram precision  2-gram precision  3-gram precision  \\\n",
       "0       0.109             0.314              0.13             0.073   \n",
       "\n",
       "   4-gram precision  rouge1  rouge2  rougeL  rougeLsum  \n",
       "0             0.047   0.346   0.149   0.252      0.323  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_inference.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(f\"./predictions_and_metric/predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "### Conversation:\n",
      "\n",
      "'Cool' credentials: This week Samantha Cameron revealed her love of alternative group Poliça . She rarely misses a chance to demonstrate her ‘cool’ credentials. And this week Samantha Cameron was at it again, revealing her love of alternative group Poliça. But their brand of psychedelic rock conjures up a world a far cry from her life in Downing Street and the Cotswolds. The American band – whose name roughly translates as ‘policy’ in Polish – are inspired by a radical feminist who described pregnancy as ‘barbaric’, and their songs feature violent imagery. The video for the first single on the group’s most recent album depicts androgynous-looking singer Channy Leaneagh subjecting her trussed-up alter ego to a violent assault. Blood spurts in all directions as she smashes her hands with a hammer, punches her in the face and finally waterboards her. Another unsettling song by the four-piece band from Minnesota, entitled Leading To Death, includes the lyric ‘I dream of you, oh my strangler’. Miss Leaneagh, 33, has described her music as ‘rhythm-driven sex noise’ and says the digital effects she uses to modify her broken-hearted vocals are ‘like taking drugs for your voice’. Her marriage broke up several years ago after she had a child, and her lyrics have a tendency towards violent, sexual imagery that bemoans ‘boys’ and ‘all the work they require’. Mrs Cameron, 43, pictured, comes from an aristocratic background and was educated at Marlborough College, whose other alumni include the Duchess of Cambridge and Princess Eugenie. But she has a distinctly bohemian streak and was friends with Bristol trip-hop artist Tricky during her time at university. She recently name-checked US indie group The War On Drugs, and claims to be a big fan of Radio 6 Music, the BBC’s ‘cutting edge and ground-breaking’ station. ‘I listen to Radio 4 in the morning and the rest of the day I have 6 Music on,’ she said in a rare interview last weekend. Inspiration: Channy Leaneagh of Polica, a band which specialises in psychedelic rock . Displaying her love of Poliça, Mrs Cameron even joined the crowd at a recent gig in Shoreditch, East London, at which the group played songs from their latest album, Shulamith. The record’s cover is illustrated with a photograph of a naked young woman, her hair and neck caked in blood. The title is a tribute to Canadian-born feminist Shulamith Firestone, whom Miss Leaneagh has described as a ‘mentor and muse from the grave’. Miss Firestone, who died in 2012 aged 67, painted a radical vision of a ‘liberated’ world without families, childbirth or any real distinctions between men and women. She is best known for her 1970 manifesto The Dialectic Of Sex: The Case For Feminist Revolution, which drew inspiration from Karl Marx’s Communist Manifesto. Miss Firestone argued that pregnancy was ‘barbaric’ and childhood was a ‘supervised nightmare’. Poliça's record cover is illustrated with a photograph of a naked young woman, her hair and neck caked in blood. The title is a tribute to Canadian-born feminist Shulamith Firestone (above) She looked forward to a new world in which women were to be liberated from men thanks to the scientific breakthroughs which meant that sexual reproduction could happen outside the womb. The radical feminist was also responsible for stunts including unfurling a ‘Women’s Liberation’ banner at a beauty pageant. Later in life she became a recluse and spent time in a mental health hospital. She was found dead in her studio flat, apparently after starving herself to death.\n",
      "\n",
      "### Summary:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(predictions.input[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samantha Cameron recently spoke of her love of alternative group Poliça .\n",
      "Rock band's latest album is inspired by feminist Shulamith Firestone .\n",
      "Mrs Cameron even joined crowd at a recent gig in Shoreditch, East London .\n"
     ]
    }
   ],
   "source": [
    "print(predictions.output[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samantha Cameron, 43, is from an aristocratic background and was educated at Marlborough College.\n",
      "She is friends with Bristol trip-hop artist Tricky during her time at university.\n",
      "She recently name-checked US indie group The War On Drugs.Theresa May has said she will not rule out a return to the European\n"
     ]
    }
   ],
   "source": [
    "print(predictions.prediction[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
