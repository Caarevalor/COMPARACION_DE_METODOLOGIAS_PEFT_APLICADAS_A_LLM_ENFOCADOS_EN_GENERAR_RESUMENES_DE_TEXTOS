{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "MODEL_NAME = \"openai-community/gpt2-large\"\n",
    "use_peft = False\n",
    "torch_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Formatting libraries\n",
    "import black\n",
    "import jupyter_black\n",
    "\n",
    "# Load jupyter_black settings\n",
    "jupyter_black.load(\n",
    "    lab=True,\n",
    "    line_length=170,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca32d1ceb7314571863f73a437735d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/4.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e7136afdeb45f599e7de43d44f6b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/11.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b30a53d9a98429c921603dea96a040b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1af81cf1b264c4c89931fb92ed9904c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99119f6a72e04b369e9ca96a43ff13c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c293d082e5d540fea76f8962e1a2fcac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e97855ff694020a94ad7d1e103254e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"knkarthick/dialogsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53829a6b21049c0807e9c124550173c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0ea60176dc4c8db60d9a5c397ddbcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ebdc65e455648cfa2edfa28829d6a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b87137859a4961abe9467421ad801c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define tokenizer. We will use the tokenizer to count the number of tokens per instance\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define prompt template\n",
    "prompt_template = \"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "### Conversation:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "### Summary:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# create prompt\n",
    "def create_prompt(data):\n",
    "    dialogue = data[\"dialogue\"]\n",
    "    summary = data[\"summary\"]\n",
    "    prompt = prompt_template.format(dialogue=dialogue, summary=summary)\n",
    "\n",
    "    n_tokens_output = len(tokenizer.encode(summary, add_special_tokens=False))\n",
    "    n_tokens_input = len(tokenizer.encode(prompt, add_special_tokens=False))\n",
    "\n",
    "    return {\"input\": prompt, \"output\": summary, \"n_tokens_input\": n_tokens_input, \"n_tokens_output\": n_tokens_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb5ac35aab542638338dfd7fa78c65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b448e05a7a834095b559149b9fce69bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1dc0d6b0b72429691fdd3e294e6db79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic', 'input', 'output', 'n_tokens_input', 'n_tokens_output'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic', 'input', 'output', 'n_tokens_input', 'n_tokens_output'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic', 'input', 'output', 'n_tokens_input', 'n_tokens_output'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(create_prompt)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "431.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get 0.95 percentile of dialogue length in training set\n",
    "dataset[\"train\"].to_pandas().n_tokens_input.quantile(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get 0.95 quantile of n_tokens_summary in train dataset`\n",
    "dataset[\"train\"].to_pandas().n_tokens_output.quantile(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f8fc233cae42a184b6bace1878eeb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56622562c1d4edcab05f6831a830d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92377c767ae94a9e9e50babaed6c343f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filter very long dialogs and summaries\n",
    "dataset = dataset.filter(lambda x: x[\"n_tokens_input\"] < 470 and x[\"n_tokens_output\"] < 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "### Conversation:\n",
      "\n",
      "#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\n",
      "#Person2#: I found it would be a good idea to get a check-up.\n",
      "#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\n",
      "#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\n",
      "#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\n",
      "#Person2#: Ok.\n",
      "#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\n",
      "#Person2#: Yes.\n",
      "#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\n",
      "#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\n",
      "#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\n",
      "#Person2#: Ok, thanks doctor.\n",
      "\n",
      "### Summary:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][\"input\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][\"output\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special Tokens: \n",
      "{'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_special_tokens=False)\n",
    "\n",
    "# verify the existing special tokens\n",
    "print(f\"Special Tokens: \\n{tokenizer.special_tokens_map}\")\n",
    "\n",
    "# if no padding token set eos_token as padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load model and tokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"cuda\", torch_dtype=torch_dtype)  # torch_dtype=torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if use_peft:\n",
    "    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=64,\n",
    "        target_modules=[\"c_attn\", \"c_proj\", \"c_fc\", \"c_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        model.enable_input_require_grads()\n",
    "    else:\n",
    "\n",
    "        def make_inputs_require_grad(module, input, output):\n",
    "            output.requires_grad_(True)\n",
    "\n",
    "        model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
    "\n",
    "    model = get_peft_model(model, config)\n",
    "    model.print_trainable_parameters()\n",
    "    model.config.use_cache = False\n",
    "else:\n",
    "    model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pad_sequence from torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataCollatorForCausalLM:\n",
    "    def __init__(self, tokenizer, source_max_len, target_max_len, training_on_source, padding_side):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.source_max_len = source_max_len\n",
    "        self.target_max_len = target_max_len\n",
    "        self.training_on_source = training_on_source\n",
    "        self.padding_side = padding_side\n",
    "\n",
    "    def __call__(self, instances):\n",
    "        source = [tokenizer.bos_token + text[\"input\"] for text in instances]\n",
    "        target = [text[\"output\"] + tokenizer.eos_token for text in instances]\n",
    "\n",
    "        source = tokenizer(\n",
    "            source,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=self.source_max_len,\n",
    "        )\n",
    "\n",
    "        target = tokenizer(\n",
    "            target,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=self.target_max_len,\n",
    "        )\n",
    "\n",
    "        input_ids = []\n",
    "        labels = []\n",
    "        for source_tokens, target_token in zip(source[\"input_ids\"], target[\"input_ids\"]):\n",
    "            if tokenizer.pad_token_id == tokenizer.eos_token_id:\n",
    "                target_token[-1] = -50\n",
    "\n",
    "            input_ids.append(torch.LongTensor(source_tokens + target_token))\n",
    "\n",
    "            if self.training_on_source:\n",
    "                labels.append(torch.LongTensor(copy.deepcopy(source_tokens + target_token)))\n",
    "            else:\n",
    "                labels.append(torch.LongTensor([-100] * len(source_tokens) + copy.deepcopy(target_token)))\n",
    "\n",
    "        # Pad sequences to the longest one in the batch\n",
    "        if self.padding_side == \"right\":\n",
    "            input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "            labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "        elif self.padding_side == \"left\":\n",
    "            # Find the maximum sequence length in the batch\n",
    "            max_length = max(len(seq) for seq in input_ids)\n",
    "\n",
    "            # Perform left padding manually\n",
    "            input_ids = [torch.cat([torch.full((max_length - len(seq),), self.tokenizer.pad_token_id, dtype=torch.long), seq]) for seq in input_ids]\n",
    "            labels = [torch.cat([torch.full((max_length - len(seq),), -100, dtype=torch.long), seq]) for seq in labels]\n",
    "\n",
    "            # Convert list of tensors to a single tensor\n",
    "            input_ids = torch.stack(input_ids)\n",
    "            labels = torch.stack(labels)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"padding_direction must be 'right' or 'left'\")\n",
    "\n",
    "        attention_mask = input_ids.ne(self.tokenizer.pad_token_id)\n",
    "\n",
    "        if tokenizer.pad_token_id == tokenizer.eos_token_id:\n",
    "            input_ids[input_ids == -50] = self.tokenizer.pad_token_id\n",
    "            labels[labels == -50] = self.tokenizer.pad_token_id\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\": attention_mask}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define collator\n",
    "data_collator = DataCollatorForCausalLM(\n",
    "    tokenizer=tokenizer,\n",
    "    source_max_len=470,\n",
    "    target_max_len=70,\n",
    "    training_on_source=False,\n",
    "    padding_side=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the training arguments for the Trainer\n",
    "training_arguments = TrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    # logging_dir=None,  # f\"./{MODEL_NAME}-tensorboard\",\n",
    "    output_dir=f\"./{MODEL_NAME}-checkpoing\",\n",
    "    num_train_epochs=10,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=10,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_steps=-1,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=5e-5,\n",
    "    remove_unused_columns=False,\n",
    "    max_grad_norm=1.0,\n",
    "    gradient_checkpointing=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    warmup_ratio=0.005,\n",
    "    logging_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    eval_steps=100,  # We use same value for logging_steps (train metrics) and  eval_steps (eval metrics).\n",
    "    group_by_length=False,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=100,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='357' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 357/1750 26:13 < 1:42:54, 0.23 it/s, Epoch 2.03/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.515500</td>\n",
       "      <td>1.137695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.954500</td>\n",
       "      <td>1.130859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the Trainer with the model, tokenizer, training arguments, datasets, and data collator\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset[\"train\"].shuffle(seed=42).select(range(7000)),\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start the training process\n",
    "trainer.train()\n",
    "\n",
    "# Save the trained model to the specified path\n",
    "trainer.model.save_pretrained(f\"./{MODEL_NAME}-peft={use_peft}-fine-tuned-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save used args on json file\n",
    "# args_json_path = os.path.join(self.new_model_name_or_path, 'training_args.json')\n",
    "# with open(args_json_path, 'w') as json_file:\n",
    "#     json.dump(args_dict, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
